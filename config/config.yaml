# --------------------------
# JOB CONFIGURATION
# --------------------------

wandb_project_folder: "CIFAR-100_centralized"         # Name of the project folder in wandb
seed: 42                                              # Random seed for reproducibility

# --------------------------
# CENTRALIZED TRAINING CONFIG
# --------------------------
batch_size: 256
val_split: 0.1
num_workers: 2
learning_rate: 0.002805291683751909
weight_decay: 0.00042327462676463
momentum: 0.9579794867057544
t_max: 50
epochs: 50
checkpoint_path: '/content/drive/MyDrive/DL_project/checkpoints/checkpoint_35.pth' # "/content/drive/MyDrive/DL_project/checkpoints/checkpoint_35.pth"           # Leave blank "" to train from scratch, or set to "checkpoints/<checkpoint_filename>.pth" to resume
out_checkpoint_dir: "/content/drive/MyDrive/DL_project/checkpoints"

finetuning_method: "dense"                             # fine-tuning method switch -> "dense" / "lora" / "talos"

# talos fine-tuning (centralized)
nesterov: False                         # enable nesterov acceleration
dampening: 0                            # fraction of grad to drop from momentum buffer
target_sparsity: 0.8                    # Target sparsity level (e.g., 80%)
sparsity_rounds: 5                      # Number of rounds for mask calibration
calib_split: 0.05                       # split of the training data dedicated to mask calibration
calib_batch_size: 16                    # for Fisher calibration
calib_rounds: 5

# LoRA fine-tuning (centralized)
lora_rank: 8                                                                # dimensionality of the low-rank update matrices (AB)
lora_alpha: 32                                                              # factor applied to the low-rank update (alpha/r * AB)
lora_dropout: 0.05                                                          # Dropout probability on the updates
lora_target_modules:
  - "qkv"
  - "proj"
  # optionally, for adapters in the MLP:
  # - "mlp.fc1"
  # - "mlp.fc2"

# --------------------------
# FEDERATED TRAINING CONFIG
# --------------------------
NUM_CLIENTS: 100
CLIENT_FRACTION: 0.1  # 10% of clients selected each round
LOCAL_EPOCHS: 5
BATCH_SIZE: 64
LR: 0.001
LR_DECAY: 0.99        # per-round global lr decay factor
WARMUP_EPOCHS: 5      # local warm-up scheduler
ROUNDS: 100
IID: True  # Set to False for non-iid
NC: 2  # Number of classes per client in non-iid setting
CHECKPOINT_PATH: '/content/drive/MyDrive/DL_project/federated_checkpoints/fl_model_round_25.pth' # "/content/drive/MyDrive/DL_project/federated_checkpoints/.pth"           # Leave blank "" to train from scratch, or set to "checkpoints/<checkpoint_filename>.pth" to resume
OUT_CHECKPOINT_PATH: "/content/drive/MyDrive/DL_project/federated_checkpoints"
FINETUNE_METHOD: 'talos'           # or "dense"

# talos fine-tuning (federated)
TALOS_TARGET_SPARSITY: 0.10       # e.g. keep 1-x % of params
TALOS_PRUNE_ROUNDS: 5           # how many mask‐calibration iterations
CALIBRATION_SPLIT: 0.1                 # fraction for Fisher‐calibration (already in place)

MASKS_DIR: "/content/drive/MyDrive/DL_project/masks_cache"      # where to save Fisher scores & masks
LOAD_MASK: ""       # "/content/drive/MyDrive/DL_project/masks_cache/xxx.pt"